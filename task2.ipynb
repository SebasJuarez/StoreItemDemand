{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271e1679",
   "metadata": {},
   "source": [
    "# Lab 8\n",
    "## TASK 2 - TEORÍA\n",
    "### Sebastian Juárez - 21471\n",
    "### Javier Prado - 21486\n",
    "### Bryan España - 21550"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a5e8a9",
   "metadata": {},
   "source": [
    "ponda claramente y con una extensión adecuada las siguientes preguntas:\n",
    "1. ¿Cuál es el problema del gradiente de fuga en las redes LSTM y cómo afecta la efectividad de LSTM para el\n",
    "pronóstico de series temporales?\n",
    "* En el gradiente de fuga, durante la retropropagación, los gradiantes qie deberian actualizar los pesos de las capas o pasos temporales lejanos se vuelven extremadamente pequeños.\n",
    "En LSTM, la celda de memoria y las compuertas fueron diseñadas precisamente para mitigar este problema; sin embargo, no lo eliminan por completo. En la práctica, el gradiente de fuga limita la “memoria efectiva” de la LSTM. Aunque el modelo pueda, en teoría, preservar información por largos intervalos, en entrenamiento los gradientes que deberían reforzar o corregir representaciones sobre dependencias de largo plazo no logran atravesar toda la cadena temporal.\n",
    "El gradiente de fuga limita la efectividad de las LSTM porque reduce su “memoria efectiva”: al retropropagar, los gradientes que deberían ajustar decisiones tomadas muchos pasos atrás se atenúan y casi no alcanzan a las capas/tiempos tempranos, por lo que la red aprende sobre todo patrones recientes y de corto plazo, mientras subrepresenta estacionalidades largas, cambios de nivel y dependencias de largo alcance; esto se traduce en degradación del rendimiento a medida que aumenta el horizonte de predicción y en entrenamientos que se estancan si no se cuidan aspectos como sesgo inicial del forget gate, normalización, gradient clipping y un preprocesamiento que quite tendencia/estacionalidad o, cuando sea necesario, el uso de arquitecturas alternativas con caminos de gradiente más cortos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ebe731",
   "metadata": {},
   "source": [
    "2. ¿Cómo se aborda la estacionalidad en los datos de series temporales cuando se utilizan LSTM para realizar\n",
    "pronósticos y qué papel juega la diferenciación en el proceso?\n",
    "* Se maneja la estacionalidad con dos enfoques: (1) deseasonalizar la serie, entrenar la LSTM sobre el residuo y luego recomponer; y/o (2) inyectar estacionalidad como features (lags 𝑦𝑡 − 𝑠 calendarios, términos de Fourier) usando ventanas que cubran ≥1–2 ciclos. La diferenciación ayuda a stationarizar: la regular quita tendencia y la estacional \n",
    "∇𝑠𝑦𝑡=𝑦𝑡 −𝑦𝑡 - s elimina el ciclo, facilitando el aprendizaje y estabilizando gradientes. Cuidado con sobre-diferenciar (añade ruido) y con invertir correctamente las transformaciones al pronosticar, evitando fugas de información en validación temporal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc2cd6b",
   "metadata": {},
   "source": [
    "3. ¿Cuál es el concepto de \"tamaño de ventana\" en el pronóstico de series temporales con LSTM y cómo afecta\n",
    "la elección del tamaño de ventana a la capacidad del modelo para capturar patrones a corto y largo plazo?\n",
    "* El **tamaño de ventana** es cuántos pasos pasados le das a la LSTM como entrada. **Ventanas cortas** aprenden bien patrones **de corto plazo** pero pierden estacionalidades y dependencias largas; **ventanas largas** capturan **patrones amplios**, pero encarecen el entrenamiento y aumentan riesgo de gradiente de fuga/overfitting.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
