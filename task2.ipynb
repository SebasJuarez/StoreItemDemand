{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271e1679",
   "metadata": {},
   "source": [
    "# Lab 8\n",
    "## TASK 2 - TEORÃA\n",
    "### Sebastian JuÃ¡rez - 21471\n",
    "### Javier Prado - 21486\n",
    "### Bryan EspaÃ±a - 21550"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a5e8a9",
   "metadata": {},
   "source": [
    "ponda claramente y con una extensiÃ³n adecuada las siguientes preguntas:\n",
    "1. Â¿CuÃ¡l es el problema del gradiente de fuga en las redes LSTM y cÃ³mo afecta la efectividad de LSTM para el\n",
    "pronÃ³stico de series temporales?\n",
    "* En el gradiente de fuga, durante la retropropagaciÃ³n, los gradiantes qie deberian actualizar los pesos de las capas o pasos temporales lejanos se vuelven extremadamente pequeÃ±os.\n",
    "En LSTM, la celda de memoria y las compuertas fueron diseÃ±adas precisamente para mitigar este problema; sin embargo, no lo eliminan por completo. En la prÃ¡ctica, el gradiente de fuga limita la â€œmemoria efectivaâ€ de la LSTM. Aunque el modelo pueda, en teorÃ­a, preservar informaciÃ³n por largos intervalos, en entrenamiento los gradientes que deberÃ­an reforzar o corregir representaciones sobre dependencias de largo plazo no logran atravesar toda la cadena temporal.\n",
    "El gradiente de fuga limita la efectividad de las LSTM porque reduce su â€œmemoria efectivaâ€: al retropropagar, los gradientes que deberÃ­an ajustar decisiones tomadas muchos pasos atrÃ¡s se atenÃºan y casi no alcanzan a las capas/tiempos tempranos, por lo que la red aprende sobre todo patrones recientes y de corto plazo, mientras subrepresenta estacionalidades largas, cambios de nivel y dependencias de largo alcance; esto se traduce en degradaciÃ³n del rendimiento a medida que aumenta el horizonte de predicciÃ³n y en entrenamientos que se estancan si no se cuidan aspectos como sesgo inicial del forget gate, normalizaciÃ³n, gradient clipping y un preprocesamiento que quite tendencia/estacionalidad o, cuando sea necesario, el uso de arquitecturas alternativas con caminos de gradiente mÃ¡s cortos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ebe731",
   "metadata": {},
   "source": [
    "2. Â¿CÃ³mo se aborda la estacionalidad en los datos de series temporales cuando se utilizan LSTM para realizar\n",
    "pronÃ³sticos y quÃ© papel juega la diferenciaciÃ³n en el proceso?\n",
    "* Se maneja la estacionalidad con dos enfoques: (1) deseasonalizar la serie, entrenar la LSTM sobre el residuo y luego recomponer; y/o (2) inyectar estacionalidad como features (lags ğ‘¦ğ‘¡ âˆ’ ğ‘  calendarios, tÃ©rminos de Fourier) usando ventanas que cubran â‰¥1â€“2 ciclos. La diferenciaciÃ³n ayuda a stationarizar: la regular quita tendencia y la estacional \n",
    "âˆ‡ğ‘ ğ‘¦ğ‘¡=ğ‘¦ğ‘¡ âˆ’ğ‘¦ğ‘¡ - s elimina el ciclo, facilitando el aprendizaje y estabilizando gradientes. Cuidado con sobre-diferenciar (aÃ±ade ruido) y con invertir correctamente las transformaciones al pronosticar, evitando fugas de informaciÃ³n en validaciÃ³n temporal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc2cd6b",
   "metadata": {},
   "source": [
    "3. Â¿CuÃ¡l es el concepto de \"tamaÃ±o de ventana\" en el pronÃ³stico de series temporales con LSTM y cÃ³mo afecta\n",
    "la elecciÃ³n del tamaÃ±o de ventana a la capacidad del modelo para capturar patrones a corto y largo plazo?\n",
    "* El **tamaÃ±o de ventana** es cuÃ¡ntos pasos pasados le das a la LSTM como entrada. **Ventanas cortas** aprenden bien patrones **de corto plazo** pero pierden estacionalidades y dependencias largas; **ventanas largas** capturan **patrones amplios**, pero encarecen el entrenamiento y aumentan riesgo de gradiente de fuga/overfitting.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
